{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7dd7b96-62ac-47b3-9cdc-6b13cc2c3b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d18aabd3-ef53-4bdc-8c36-9770adc0e3ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae9f2d14-0d33-4a09-aca0-6f2638699aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7696ccd-de47-4db3-9191-93a2b52682bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42a54ba8-85a2-4194-a807-b2936ff30bb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It', 'is', 'not', 'news', 'that', 'Nathan', ...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_words = brown.words(categories = 'reviews')\n",
    "reviews_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6eebfd3-2b9f-4d91-892e-a9d8c6ae6f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40704"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d275607-0c82-440c-a486-39e71519f89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['As', 'a', 'result', ',', 'although', 'we', 'still', ...]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "39399"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_words = brown.words(categories = 'religion')\n",
    "print(reviews_words)\n",
    "len(reviews_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdcfd99-38b6-44a6-af0e-43a24d8c66f8",
   "metadata": {},
   "source": [
    "#TOKENIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4541a4ca-238d-4006-a1fd-583b4e208f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2827f8b4-28b2-48f8-9720-a16fd083749e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello World!', 'Be Greateful for what you have.']\n",
      "['Hello', 'World', '!', 'Follow', 'the', 'path', 'of', 'Allah', '.']\n"
     ]
    }
   ],
   "source": [
    "Ex_sent = \"Hello World! Be Greateful for what you have.\"\n",
    "Ex_word = \"Hello World! Follow the path of Allah.\"\n",
    "print(sent_tokenize(Ex_sent))\n",
    "print(word_tokenize(Ex_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c8706e-d947-4c21-bae6-f67bc98ad3d2",
   "metadata": {},
   "source": [
    "#STOP WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c944850b-236d-49d9-bbce-da507caf6c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98e4c312-d2bd-4c68-a66c-a382466927dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'day', 'keeps', 'doctor', 'away']\n"
     ]
    }
   ],
   "source": [
    "example_sentence = \"an apple a day keeps the doctor away\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "word_tokens = word_tokenize(example_sentence)\n",
    "\n",
    "filtered_sentence = []\n",
    "\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "        \n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05d37ef-ad09-4768-a63f-8fe51a3b4dfe",
   "metadata": {},
   "source": [
    "#STEMMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "958f2657-af20-4f25-8a24-41c1cdb2f23c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import\n",
      "of\n",
      "cave\n",
      "is\n",
      "explain\n",
      "by\n",
      "caver\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "ps = PorterStemmer()\n",
    "sentence = \"importance of caving is explained by cavers\"\n",
    "\n",
    "words = word_tokenize(sentence)\n",
    "for w in words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6023bbc-2c52-4f85-97d7-67e153fe3189",
   "metadata": {},
   "source": [
    "#LEMMATIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a2c76e62-45d6-4292-b1ab-38f339c8cb32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foot\n",
      "loving\n",
      "love\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize('feet'))\n",
    "\n",
    "# without a POS tag lemmatizer considers every word a noun\n",
    "print(lemmatizer.lemmatize('loving'))\n",
    "# with a POS tag\n",
    "print(lemmatizer.lemmatize('loving','v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c60ed0-64e9-4e05-b364-5fe9d01e24cc",
   "metadata": {},
   "source": [
    "#NAMER ENTITY RECOGNIZATION (NET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a3131b3c-ca68-47d2-9844-182ac77825eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "para = '''Google is a multinational technology company that specializes in\n",
    "        internet-related services and products. The company was founded in \n",
    "        1998 by Larry Page and Sergey Brin while they were Ph.D. students \n",
    "        at Stanford University.'''\n",
    "#Tokenize Data\n",
    "tokenized_para = sent_tokenize(para)\n",
    "tagged_sentences = nltk.pos_tag(tokenized_para)\n",
    "ne_chunked_sents = nltk.ne_chunk(tagged_sentences)\n",
    "\n",
    "#extract all named entities\n",
    "named_entities = []\n",
    "for tagged_tree in ne_chunked_sents:\n",
    "    if hasattr(tagged_tree, 'label'):\n",
    "        entity_name = ' '.join(c[0] for c in tagged_tree.leaves())\n",
    "        entity_type = tagged_tree.label()\n",
    "        named_entities.append((entity_name,entity_type))\n",
    "\n",
    "print(named_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71f45d7-da9a-4ce0-a13a-8f2259473e29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
